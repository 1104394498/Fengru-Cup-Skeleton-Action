![image-20200205211323810](C:\Users\94717\AppData\Roaming\Typora\typora-user-images\image-20200205211323810.png)

- UT-Kinect数据集: http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html

  通过固定的使用一个固定的Kinect和Kinect for Windows SDK Beta版本的深度相机以15 fps的帧速率收集数据，包含RGB，Depth和3Dskeleton数据。UT-Kinect将样本分为10种日常生活行为，包括 走路，坐下，站起来，拿起，携带，扔，推，拉，挥手，拍手 （walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands）等。这些行动由10个不同的人执行，同一行动每个人进行两次。该数据集中总共包含199个动作序列。
  
- Florence 3D 数据集(2012年的): Kinect收集数据，收集九个常见的室内动作类别，如“观看”，“饮用水”，“呼叫”等。在这些行动中，10个人完成9个动作，每个动作重复执行2或3次，总计215次动作。与UT-Kinect数据集相比，佛罗伦萨3D数据集不仅存在较大的类内差异，而且在不同类之间的较少的相互间变化中也很困难。例如，从骨骼动作序列的角度来看，“观看”，“饮用水”和“呼叫”是类似的。

  http://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/

  

- **CMU Panoptic dataset**

   http://domedb.perception.cs.cmu.edu/tutorials/cvpr17/index.html
       该数据集是CMU大学制作，由480个VGA摄像头，30+HD摄像头和10个Kinnect传感器采集。

  size： 65 sequences (5.5 hours) and 1.5 millions of 3D skeletons are available

  CMU MoCap Dataset包含了3D人体关键点标注和骨架移动标注的数据集 ?(来自网络)

  

- **MSRAction3D数据集**

  https://zhuanlan.zhihu.com/p/63133269

  该数据集记录了20种动作actions，10个被试者subjects，每个对象执行每个动作2-3次。总共567个深度图序列，分辨率为640*240，用类似于Kinect装置的深度传感器记录数据。

  

  

  *人体姿态估计数据集   https://www.jianshu.com/p/c046db584a21

  =================================================================

  3D数据集：

- Human3.6M数据集（提取部分用到的）
     Human3.6M数据集有360万个3D人体姿势和相应的图像，共有11个实验者（6男5女，论文一般选取1，5，6，7，8作为train，9，11作为test），共有17个动作场景，诸如讨论、吃饭、运动、问候等动作。

  

- MPI-INF-3DHP
       该数据集由Max Planck Institute for Informatics制作，详情可见Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision论文

  

  等等。。。。。具体见链接

  

  

- 2D数据集：Pose Estimation

  LSP

  地址：http://sam.johnson.io/research/lsp.html

  样本数：2K 	关节点个数：14	全身，单人

  。。。。。具体见链接

  

  ======================  other dataset (video+类别)====================

  - The UT-interaction dataset（video no skeleton)

    针对交互行为的数据库，包含有6类人人交互的动作(shaking hands, pointing, hugging,pushing, kicking and punching)总共 20 段样本,长度在 1 min 左右。平均每个视频执行8次人类活动，为这些交互提供了Ground truth标签，包括时间间隔和边界框。

    下载地址：http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html

    

  - The UCF Dataset

  - 这些数据库样本来自从 BBC/ESPN的广播电视频道收集的各类运动样本、以及从互联网尤其是视频网站YouTube上下载而来的样本。其中UCF101是目前动作类别数、样本数最多的数据库之一，样本为13320段视频，类别数为101类。

    下载地址：http://crcv.ucf.edu/data/

    

- **一个大全**：http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#human



+++++++++++++++++++++++++++++补充recognition



//Berkeley MHAD

 SBU Kinect Interaction, 

NTU RGB-D datasets、

UTKinect-Action3D

Kinetics

HDM05 datasets

UTD-MHAD Dataset

**UTD-MHAD Dataset**



## 论文三：2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning

这是今年CVPR 2018中利用姿态做行为识别的一篇文章，主要突出了一个多任务网络来同时做2D和3D的姿态估计以及2D和3D的行为识别，同时利用姿态估计的结果来促进行为识别任务的性能。**这也是解决问题的一个很好的出发点，就是利用两个任务来互相促进**。



下图是网络的整体框架图，输入静态的RGB图像，同时进行姿态估计和行为识别。其中的姿态估计模型是利用基于回归的方法，其中利用了一个可微分的Softargmax来联合2D和3D的姿态估计。其中的动作识别方法分为两部分，一部分基于身体关节坐标序列，我们称之为**基于姿态的识别**，另一部分基于一系列视觉特征，我们称其为**基于外观的识别**。 将每个部分的结果组合起来估计最终的动作标签。



作者在MPII, Human3.6M, Penn Action 和 NTU四个数据集上进行了实验，验证了模型在两个任务上的有效性。



本文值得借鉴的一个思想就是：利用多任务之间的互相促进，来提升各自任务的有效性。